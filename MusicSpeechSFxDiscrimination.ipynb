{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music/Speech/SFx Discrimination based on VGGish and Transfer Learning\n",
    "\n",
    "*Note:* Before following this notebook, please see the simpler Music/Speech discrimination model first since it's much\n",
    "\n",
    "This notebook adapts [VGGish](https://github.com/tensorflow/models/tree/master/research/audioset) to discriminate between Music, Speech and Sound Effects. This model is a pre-trained model architecture initially used for image tasks but trained on a huge amount of data available from the (AudioSet)[https://research.google.com/audioset/] dataset which consists of soundtracks from 8 million youtube videos classified in 632 classes. The model architecture might not be the best chosen (assumes symmetric filters which make no sense when dealing with spectral representations of audio) but the amount of data is immense. The model is described in more detail in:\n",
    "\n",
    "```\n",
    "E.T. Chourdakis, L. Ward, M. Paradis, and J. D. Reiss\n",
    "\"Modelling experts' decisions on assigning narrative importances of objects in a radio drama mix\"\n",
    "[submitted] In Proc. Int. Conf. Digital Audio Effects (DAFx)\n",
    "            Sept 2019, Birmigham, UK\n",
    "```\n",
    "\n",
    "Roughly the method it classifies files is:\n",
    "\n",
    "1. Split audio files to segments of 960ms\n",
    "2. Compute mel spectrum magnitudes (64bands, 25ms frames, 10ms hop size)\n",
    "3. Keep the convolutional layers of VGGish with their weights and throw the rest\n",
    "4. Add two new fully connected neural layers\n",
    "\n",
    "Additionally, this time we use the [GTZAN](http://marsyas.info/downloads/datasets.html) dataset but we also added an equal amount of time of sound effects from the recently released [BBC SFx library](http://bbcsfx.acropolis.org.uk/). See the paper for more info on the construction of the dataset.\n",
    "\n",
    "## Additional Credits:\n",
    "\n",
    "- https://github.com/antoinemrcr/vggish2Keras for `vggish_keras.py`\n",
    "- https://github.com/tensorflow/models/tree/master/research/audioset for the preprocessing and input steps\n",
    "- https://github.com/DTaoo/VGGish for the pre-trained model\n",
    "\n",
    "## Download important files\n",
    "\n",
    "This workbook requires some supplemental python files:\n",
    "    1. Go to https://github.com/DTaoo/VGGish and download the model with the top fully connected layers, place it in the same directory as this notebook.\n",
    "    2. Run the cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  3314  100  3314    0     0  11669      0 --:--:-- --:--:-- --:--:-- 11669\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  9874  100  9874    0     0  31851      0 --:--:-- --:--:-- --:--:-- 31749\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2028  100  2028    0     0   7294      0 --:--:-- --:--:-- --:--:--  7294\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  3868  100  3868    0     0  13963      0 --:--:-- --:--:-- --:--:-- 13963\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1300  100  1300    0     0   4832      0 --:--:-- --:--:-- --:--:--  4832\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -O https://raw.githubusercontent.com/tensorflow/models/master/research/audioset/vggish_input.py\n",
    "curl -O https://raw.githubusercontent.com/tensorflow/models/master/research/audioset/mel_features.py\n",
    "curl -O https://raw.githubusercontent.com/tensorflow/models/master/research/audioset/vggish_params.py\n",
    "curl -O https://raw.githubusercontent.com/tensorflow/models/master/research/audioset/vggish_postprocess.py\n",
    "curl -O https://raw.githubusercontent.com/antoinemrcr/vggish2Keras/master/vggish_keras.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initially, import packages and load dataset\n",
    "\n",
    "Like in the case of simple Music/Speech discrimination, we load the data, this time with the addition of sound effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Music Speech Discrimination part\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import keras\n",
    "import keras.layers\n",
    "from keras.optimizers import Adam, Adadelta, RMSprop, SGD\n",
    "from keras.models import Sequential\n",
    "import librosa\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "set_random_seed(1) # For reproducibility in the model training\n",
    "np.random.seed(1)  # For reproducibility in the dataset collection\n",
    "\n",
    "music_data_dir = \"../dataset/music_speech/music_wav/\"\n",
    "speech_data_dir = \"../dataset/music_speech/speech_wav/\"\n",
    "sfx_data_dir = \"../dataset/music_speech/sfx_wav/\"\n",
    "\n",
    "\n",
    "full_data = []\n",
    "\n",
    "# 0 for music, 1 for speech\n",
    "for fname in glob.glob(os.path.join(music_data_dir, \"*.wav\")):\n",
    "    full_data.append((fname, 0))\n",
    "for fname in glob.glob(os.path.join(speech_data_dir, \"*.wav\")):\n",
    "    full_data.append((fname, 1))\n",
    "for fname in glob.glob(os.path.join(sfx_data_dir, \"*.wav\")):\n",
    "    full_data.append((fname, 2))\n",
    "    \n",
    "# Use 30% as test data in this case\n",
    "train_data, test_data = train_test_split(full_data, test_size=0.3, random_state=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines load the original VGGish weights and required parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Transfer learning part\n",
    "import vggish_input\n",
    "import vggish_params\n",
    "import vggish_postprocess\n",
    "import vggish_keras\n",
    "\n",
    "\n",
    "# Paths to downloaded VGGish files.\n",
    "checkpoint_path = 'vggish_audioset_weights.h5'\n",
    "pca_params_path = 'vggish_pca_params.npz'\n",
    "\n",
    "# Relative tolerance of errors in mean and standard deviation of embeddings.\n",
    "rel_error = 0.1  # Up to 10%\n",
    "\n",
    "# Load model for transfer learning\n",
    "vggish_model = vggish_keras.get_vggish_keras()\n",
    "vggish_model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we process the input before feeding them to the neural network. It is not straightforward to use Kapre here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing inputs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba86e90cd0e42e18e5fccd07337284f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=138), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm \n",
    "\n",
    "def batch_preprocess_files(train_data):\n",
    "    # Load data\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    print(\"Preparing inputs\")\n",
    "    for train_tuple in tqdm.tqdm_notebook(train_data):\n",
    "        src, sr = librosa.load(train_tuple[0], sr=None, mono=True)\n",
    "        label = train_tuple[1]\n",
    "\n",
    "        input_batch = vggish_input.waveform_to_examples(src, sr)\n",
    "\n",
    "        X_train.append(input_batch)\n",
    "\n",
    "        for I in range(0,input_batch.shape[0]):\n",
    "            if label == 0:\n",
    "                y_train.append(np.array([1,0,0]))\n",
    "            elif label == 1:\n",
    "                y_train.append(np.array([0,1,0]))\n",
    "            else:\n",
    "                y_train.append(np.array([0,0,1]))\n",
    "\n",
    "    X_train = np.vstack(X_train)\n",
    "    y_train = np.vstack(y_train)\n",
    "    return X_train, y_train\n",
    "\n",
    "# Do preprocessing for training\n",
    "X_train, y_train = batch_preprocess_files(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing inputs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6e70d29fbc4ca98ce7d5162d654227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=60), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Do preprocessing for testing\n",
    "X_test, y_test = batch_preprocess_files(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 96, 64, 64)        640       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 48, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 48, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 24, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             (None, 24, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv3_2 (Conv2D)             (None, 24, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 12, 8, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             (None, 12, 8, 512)        1180160   \n",
      "_________________________________________________________________\n",
      "conv4_2 (Conv2D)             (None, 12, 8, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 6, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "fc1_1 (Dense)                (None, 4096)              50335744  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1048832   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 55,885,059\n",
      "Trainable params: 1,049,603\n",
      "Non-trainable params: 54,835,456\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Construct model \n",
    "model = Sequential()\n",
    "\n",
    "# Add the convolutional layers of VGGish to the\n",
    "# newly created model \n",
    "for layer in vggish_model.layers[:-2]:\n",
    "    model.add(layer)\n",
    "    \n",
    "# Freeze their weights\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a fully connected layer with dropout (for regularization)\n",
    "# as well as a final classification layer with the softmax activation function\n",
    "# so that it gives us a categorical probability mass function\n",
    "model.add(keras.layers.Dense(256, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.9))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we do the actual training. For GPUs you can increase the batch size and the number of epochs. Since I am using a cpu we keep those values low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 3718 samples, validate on 414 samples\n",
      "Epoch 1/4\n",
      "3718/3718 [==============================] - 60s 16ms/step - loss: 0.2864 - acc: 0.9013 - val_loss: 0.0706 - val_acc: 0.9855\n",
      "Epoch 2/4\n",
      "3718/3718 [==============================] - 60s 16ms/step - loss: 0.0837 - acc: 0.9750 - val_loss: 0.0440 - val_acc: 0.9879\n",
      "Epoch 3/4\n",
      "3718/3718 [==============================] - 60s 16ms/step - loss: 0.0500 - acc: 0.9866 - val_loss: 0.0381 - val_acc: 0.9903\n",
      "Epoch 4/4\n",
      "3718/3718 [==============================] - 60s 16ms/step - loss: 0.0406 - acc: 0.9911 - val_loss: 0.0268 - val_acc: 0.9928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbfa3917240>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.0001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9), metrics=['accuracy'])\n",
    "model.fit(X_train[:,:,:,None], y_train, validation_split=0.1, epochs=4, batch_size=16, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as in the simple Music/Speech classification, report for testing. \n",
    "\n",
    "**Note** Running it on GPU with `Cuda/Cudnn=7.5.0` consistently gives different results than when running on cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       music       0.94      0.99      0.96       713\n",
      "      speech       1.00      0.96      0.98       527\n",
      "         sfx       0.95      0.91      0.93       519\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1759\n",
      "   macro avg       0.96      0.95      0.96      1759\n",
      "weighted avg       0.96      0.96      0.96      1759\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on testing dataset\n",
    "y_pred = model.predict(X_test[:,:,:,None])\n",
    "\n",
    "# Convert 0 to music, 1 to speech, 1 to sfx\n",
    "labels = ['music', 'speech', 'sfx']\n",
    "y_test_labels = [labels[i] for i in np.argmax(y_test, axis=1)]\n",
    "y_pred_labels = [labels[i] for i in np.argmax(y_pred, axis=1)]\n",
    "\n",
    "print(classification_report(y_test_labels, y_pred_labels, labels=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also show the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbf93bae2e8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_mtx = confusion_matrix(y_test_labels, y_pred_labels, labels=labels)\n",
    "\n",
    "# https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix \n",
    "cf_df = pd.DataFrame(cf_mtx, index=labels, columns=labels)\n",
    "sns.heatmap(cf_df, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, feed the testing data to save a full nice usuable model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can't save the model with the save function (I am unsure why, something to do with keras and input shapes) so we will save it as two files, a `.json` with the model architecture and a `.h5` with the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('music_speech_sfx_discriminator.json', 'w') as f:\n",
    "    f.write(model_json)\n",
    "    \n",
    "model.save_weights('music_speech_sfx_discriminator.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
